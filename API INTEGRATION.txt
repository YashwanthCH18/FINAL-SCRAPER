The Complete Scraper Service API Guide
This guide outlines the two primary endpoints of the Scraper Service and how the frontend should interact with them.

---

### Journey 1: New User Onboarding & Automatic Profile Analysis

**Goal:** After a new user completes their onboarding questions, the system automatically scrapes content based on their core topic to build their foundational "Content DNA."

**Endpoint 1: POST /scraper/profile**

*   **What it does:** Instructs the Scraper Service to perform a targeted web scrape based on the user's core topic of interest (from onboarding question #3).
*   **Why it's required:** This is the engine of the automatic analysis. It gathers the raw data needed to understand the user's brand voice for all future automated content generation.
*   **Frontend Integration:**
    *   **Where:** This is used immediately after the user successfully submits their 5 onboarding questions.
    *   **Flow:**
        1.  The user fills out the 5 questions and clicks "Complete Setup".
        2.  The frontend first makes a call to save the onboarding data to the database.
        3.  Upon a successful response for saving the data, the frontend **immediately** makes a second, separate call to `POST /scraper/profile`.
        4.  This call requires only the user's JWT in the `Authorization` header. The backend will automatically find the user's topic.
        5.  The API will respond instantly with `{"status": "scraping_started"}`. The frontend can then navigate the user to their dashboard.

---

### Journey 2: Manual Content Creation (On-Demand Research)

**Goal:** Allow the user to perform research on any topic, which can then be used to generate specific content like blog posts or social media updates.

**Endpoint 2: POST /scraper/topic**

*   **What it does:** Instructs the Scraper Service to perform a targeted web scrape for a specific topic provided by the user.
*   **Why it's required:** This is the starting point for all on-demand content. It gathers fresh, relevant, third-party context for the AI to use.
*   **Frontend Integration:**
    *   **Where:** On any page where a user can generate new content. There will be a text input field for their topic/prompt.
    *   **Button:** This call is triggered by the first action button, e.g., "Gather Research" or "Analyze Topic".
    *   **Flow:**
        1.  The user types their desired topic (e.g., "SaaS pricing strategies in 2025") into the input field.
        2.  They click the "Gather Research" button.
        3.  The frontend takes the text from the input field and sends it in the body of a `POST` request to `/scraper/topic`.
        4.  The API will respond almost instantly with `{"status": "scraping_started"}`.
        5.  The UI should now change to a "researching" state. For example, disable the input field and show a message like "Research in progress... You can generate content in a minute."